# Deep Auto-encoder

## Auto-Encoder

<img src="images/image-20231206002049130.png" alt="image-20231206002049130" style="zoom:50%;" />

- the encoder generates a compact representation of the input object
- the decoder can reconstruct original object
- both of them learn together

### Review: Principal Component Analysis PCA

- PCA is a technique used in statistics and machine learning to simplify the complexity in high-dimensional data while retaining trends and patterns

<img src="images/image-20231206002354661.png" alt="image-20231206002354661" style="zoom:50%;" />

### Deep Auto-encoder

<img src="images/image-20231206002431489.png" alt="image-20231206002431489" style="zoom:75%;" />

- note that the symmetry is not necessary

### De-noising auto-encoder

<img src="images/image-20231206002830872.png" alt="image-20231206002830872" style="zoom:50%;" />

- add noise to the input image before encoding
- this makes the encoding more robust

---

## Auto-encoder for CNN

<img src="images/image-20231206003230314.png" alt="image-20231206003230314" style="zoom:50%;" />

### CNN: Unpooling

- when performing max pooling, save *max location switches*
- when reconstructing use the switches to unpool 

<img src="images/image-20231206003537213.png" alt="image-20231206003537213" style="zoom:50%;" />

- alternative is to simply repeat the values 

### CNN: Deconvolution

- actually, deconvolution *is* a convolution
- simply add zero-padding so that the output is the same shape as the input

<img src="images/image-20231206003738535.png" alt="image-20231206003738535" style="zoom:50%;" />

- where the left is a convolution
  - the middle is a deconvolution
  - and the right is the equivalent convolution to the deconvolution




